# Глава 2: Основы RAG: Retrieval-Augmented Generation

В этой главе вы углубите свои знания о Retrieval-Augmented Generation (RAG), ключевой технологии для интеграции внешних данных с большими языковыми моделями (LLM). Мы разберем принципы работы RAG, его архитектуру, сравним подходы LangChain и LlamaIndex, а также создадим RAG-пайплайны с акцентом на практическое применение. А также, мы дополним теорию практическими примерами, упражнениями и рекомендациями по оптимизации и рассмотрим продвинутые аспекты RAG, такие как настройка ретриверов и выбор подходящих эмбеддингов, чтобы вы могли эффективно применять эту технологию в реальных проектах.

## 2.1 Что такое RAG и как он работает?

Retrieval-Augmented Generation (RAG) — это гибридный подход, объединяющий поиск релевантных данных (retrieval) и генерацию текста с помощью больших языковых моделей (generation). RAG позволяет LLM использовать внешние источники данных, такие как документы, базы знаний или корпоративные архивы, для формирования более точных и контекстно-релевантных ответов. Это особенно важно для задач, где модель должна опираться на актуальную или специализированную информацию, недоступную в ее обучающих данных.

**Как работает RAG?**
1. **Этап поиска (Retrieval)**: Ретривер, используя эмбеддинги запроса, ищет наиболее релевантные документы в векторном хранилище (например, FAISS). Эмбеддинги создаются с помощью моделей, таких как OpenAI Embeddings или Hugging Face Sentence Transformers.
2. **Этап генерации (Generation)**: Найденные документы включаются в промпт LLM, которая генерирует ответ, опираясь на предоставленный контекст.
3. **Оптимизация (опционально)**: Для повышения качества могут использоваться переранжировка результатов поиска, фильтрация или постобработка ответов.

RAG минимизирует проблему галлюцинаций (когда модель генерирует недостоверные данные), так как ответы основаны на реальных документах. Это делает RAG идеальным для приложений, требующих точности, например, в клиентской поддержке, юридическом анализе или научных исследованиях.

**Пример**: Базовый RAG-пайплайн с использованием LangChain и FAISS для ответа на вопрос по документам.

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# Набор документов
docs = ["ИИ помогает автоматизировать бизнес-процессы.", "Чат-боты улучшают клиентский сервис."]
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(docs, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})  # Указываем количество возвращаемых документов
llm = ChatOpenAI(model="gpt-3.5-turbo")
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="Контекст: {context}\nВопрос: {question}\nОтвет:"
)
chain = {"context": retriever, "question": lambda x: x} | prompt | llm
response = chain.invoke("Как ИИ помогает бизнесу?")
print(response.content)
```

**Дополнение**: Для повышения точности можно настроить параметры ретривера, например, использовать `search_type="mmr"` (Maximum Marginal Relevance) в FAISS для разнообразия результатов поиска или увеличить `k` для большего контекста. Также важно учитывать качество эмбеддингов: модели с поддержкой русского языка, такие как `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, могут улучшить релевантность поиска для русскоязычных документов.

## 2.2 Компоненты RAG-пайплайна

RAG-пайплайн включает три основных компонента:
1. **Ретривер**: Отвечает за поиск релевантных документов. Использует эмбеддинги для преобразования запроса и документов в векторное пространство, затем применяет алгоритмы поиска (например, k-NN в FAISS).
2. **Векторное хранилище**: Хранит эмбеддинги документов для быстрого поиска. FAISS, благодаря своей скорости и поддержке больших объемов данных, является популярным выбором для продакшен-систем.
3. **Генеративная модель (LLM)**: Использует контекст из ретривера для генерации ответа. Промпт обычно включает найденные документы и запрос пользователя.

**Пример**: Создание RAG-пайплайна с LlamaIndex и FAISS для индексации и поиска.

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.core import StorageContext
from langchain_openai import OpenAIEmbeddings
import faiss

# Загрузка документов
documents = SimpleDirectoryReader("data").load_data()
# Инициализация FAISS
faiss_index = faiss.IndexFlatL2(1536)  # Размерность эмбеддингов OpenAI
vector_store = FaissVectorStore(faiss_index=faiss_index)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
# Создание индекса
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=OpenAIEmbeddings())
query_engine = index.as_query_engine()
response = query_engine.query("Какой продукт выпускает компания?")
print(response)
```

**Дополнение**: FAISS поддерживает различные типы индексов, такие как `IndexFlatL2` (точный поиск) или `IndexIVFFlat` (приближенный поиск для больших данных). Выбор индекса зависит от объема данных и требований к скорости/точности. Для продакшен-систем рекомендуется использовать `IndexIVFFlat` с предварительным обучением на выборке данных для ускорения поиска.

## 2.3 Сравнение подходов LangChain и LlamaIndex

LangChain и LlamaIndex — два популярных фреймворка для реализации RAG, но их подходы различаются:
- **LangChain**:
  - Фокусируется на создании гибких цепочек (chains) и агентов, которые могут комбинировать ретриверы, LLM и дополнительные инструменты (например, память или внешние API).
  - Подходит для сложных сценариев, где требуется кастомная логика обработки данных или интеграция с несколькими источниками.
  - Пример: создание агента, который сначала ищет данные в FAISS, затем проверяет их через SQL-базу.
- **LlamaIndex**:
  - Оптимизирован для эффективной индексации и поиска данных, особенно при работе с большими объемами неструктурированных данных.
  - Удобен для задач, где основной акцент — на качестве и скорости ретривера.
  - Пример: индексация корпоративных документов для быстрого ответа на запросы.

**Как выбрать?** Для задач с интенсивной индексацией (например, обработка тысяч документов) LlamaIndex может быть предпочтительнее благодаря встроенным оптимизациям. LangChain лучше подходит для сценариев, где требуется сложная логика или интеграция с внешними системами. В реальных проектах часто комбинируют оба подхода: LlamaIndex для индексации, LangChain для построения цепочек.

**Дополнение**: При выборе между LangChain и LlamaIndex учитывайте совместимость с русскоязычными данными. Например, LlamaIndex поддерживает кастомные эмбеддинги, что упрощает использование моделей, таких как `sentence-transformers`, для русского языка. LangChain требует больше ручной настройки для аналогичных задач, но его модульность позволяет интегрировать сторонние библиотеки, такие как Hugging Face.

## 2.4 Преимущества и ограничения RAG

**Преимущества**:
- **Точность**: RAG использует внешние данные, что снижает вероятность галлюцинаций и повышает достоверность ответов.
- **Актуальность**: Позволяет работать с новыми или специфическими данными без переобучения модели.
- **Экономия ресурсов**: Не требует дообучения LLM, что снижает вычислительные затраты.
- **Гибкость**: Поддерживает различные источники данных (текст, PDF, базы данных) и типы ретриверов.

**Ограничения**:
- **Качество ретривера**: Неточные эмбеддинги или плохо настроенный поиск могут привести к нерелевантным результатам.
- **Скорость**: Поиск в больших индексах может быть медленным без оптимизации (например, с помощью FAISS `IndexIVFFlat`).
- **Объем контекста**: LLM имеет ограничение на длину входного текста, что может потребовать усечения найденных документов.
- **Зависимость от данных**: Качество ответов зависит от полноты и актуальности индексированных данных.

**Оптимизация**:
- Используйте высококачественные эмбеддинги, такие как `text-embedding-ada-002` от OpenAI или мультиязычные модели от Hugging Face.
- Настройте параметры ретривера (например, `k` или `search_type`) для баланса между точностью и скоростью.
- Применяйте постобработку, например, переранжировку результатов с помощью моделей, таких как `cross-encoder/ms-marco-MiniLM-L-12-v2`.

## 2.5 Практическое применение RAG в Data Science

RAG активно применяется в Data Science для решения задач, связанных с обработкой неструктурированных данных:
- **Клиентская поддержка**: Чат-боты на основе RAG отвечают на вопросы клиентов, опираясь на каталоги, FAQ или внутренние базы знаний.
- **Анализ документов**: RAG помогает извлекать информацию из контрактов, научных статей или отчетов, автоматизируя анализ.
- **Автоматизация отчетности**: Генерация отчетов на основе данных из корпоративных систем.
- **E-commerce**: Ответы на вопросы о продуктах на основе описаний или отзывов.
- **Финтех**: Анализ нормативных документов или клиентских запросов.

**Пример**: RAG для ответа на вопрос о продуктах компании с LangChain и FAISS.

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

docs = ["Продукт A: чат-бот для поддержки.", "Продукт B: платформа для анализа данных."]
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(docs, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
llm = ChatOpenAI(model="gpt-3.5-turbo")
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="Контекст: {context}\nВопрос: {question}\nОтвет:"
)
chain = {"context": retriever, "question": lambda x: x} | prompt | llm
response = chain.invoke("Что такое Продукт A?")
print(response.content)
```

**Дополнение**: Для продакшен-приложений рекомендуется добавлять постобработку ответов, например, проверять их на соответствие корпоративным стандартам или фильтровать по ключевым словам. Также можно использовать несколько ретриверов (например, гибридный поиск с FAISS и BM25) для повышения качества.

## Упражнения

1. **Создание RAG с LangChain и FAISS**: Настройте RAG-пайплайн с использованием FAISS и OpenAI для ответа на вопрос "Что такое чат-бот?" на основе документов: "Чат-боты — это программы для общения с клиентами." и "Чат-боты используют ИИ для автоматизации."
2. **RAG с LlamaIndex и FAISS**: Создайте папку `data` с текстовым файлом, содержащим описание двух продуктов (3-4 предложения). Проиндексируйте его с использованием FAISS и выполните запрос "Какой функционал у продуктов?".
3. **Оптимизация ретривера**: Используйте FAISS с `search_type="mmr"` и без него для одного и того же набора документов ("ИИ улучшает эффективность.", "ИИ снижает затраты."). Сравните ответы на вопрос "Как ИИ влияет на бизнес?" и объясните различия.
4. **Гибридный поиск**: Реализуйте RAG-пайплайн с LangChain, используя FAISS и BM25 (из `rank_bm25`) для одного набора документов. Сравните результаты на вопрос "Как ИИ помогает бизнесу?".

## Рекомендации по выполнению

- Понадобятся библиотеки `langchain`, `llama-index`, `langchain-openai`, `langchain-community`, `faiss-cpu`, `rank_bm25`.
- Используте API-ключ OpenAI через переменную окружения (`OPENAI_API_KEY`).
- Для упражнения 2 создайте папку `data` и добавьте текстовый файл перед выполнением.
- Для русскоязычных данных протестируйте эмбеддинги, такие как `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, чтобы улучшить релевантность поиска.
- При оптимизации FAISS используйте `IndexIVFFlat` для больших данных и настройте параметр `nprobe` для баланса скорости и точности.